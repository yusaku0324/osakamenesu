#!/usr/bin/env python3
"""
å“è³ªç›£è¦–ã¨ã‚²ãƒ¼ãƒˆã‚­ãƒ¼ãƒ‘ãƒ¼ - è£½å“ã®å“è³ªã‚’è‡ªå‹•çš„ã«ç¶­æŒ
"""

import json
import subprocess
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple
import requests

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class QualityGate:
    """å“è³ªã‚²ãƒ¼ãƒˆã¨è‡ªå‹•æ”¹å–„ã‚’ç®¡ç†"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.metrics = {
            "code_coverage": 85,      # æœ€ä½85%ã®ã‚³ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸
            "performance_score": 90,   # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢90ç‚¹ä»¥ä¸Š
            "security_score": 95,      # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢95ç‚¹ä»¥ä¸Š
            "user_satisfaction": 4.5,  # ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦4.5ä»¥ä¸Š
            "bug_density": 0.1,        # ãƒã‚°å¯†åº¦0.1ä»¥ä¸‹
            "technical_debt": 5        # æŠ€è¡“çš„è² å‚µ5%ä»¥ä¸‹
        }
        self.results = {}
    
    def automated_review(self) -> Dict:
        """è‡ªå‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å®Ÿè¡Œ"""
        logger.info("ğŸ” è‡ªå‹•å“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™")
        
        self.results['timestamp'] = datetime.now().isoformat()
        self.results['tests'] = self.run_automated_tests()
        self.results['security'] = self.security_scan()
        self.results['performance'] = self.performance_test()
        self.results['code_quality'] = self.analyze_code_quality()
        self.results['documentation'] = self.check_documentation()
        
        # å…¨ä½“çš„ãªå“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        self.results['overall_score'] = self._calculate_overall_score()
        
        # å“è³ªã‚²ãƒ¼ãƒˆã®åˆ¤å®š
        if self._passes_quality_gate():
            logger.info("âœ… å“è³ªã‚²ãƒ¼ãƒˆã‚’é€šéã—ã¾ã—ãŸ")
            self.deploy_to_production()
        else:
            logger.warning("âŒ å“è³ªã‚²ãƒ¼ãƒˆã‚’é€šéã§ãã¾ã›ã‚“ã§ã—ãŸ")
            self.create_improvement_tasks()
        
        return self.results
    
    def run_automated_tests(self) -> Dict:
        """è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        logger.info("ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œä¸­...")
        
        result = subprocess.run(
            ["python", "-m", "pytest", "tests/", "--cov=./", "--cov-report=json"],
            capture_output=True,
            text=True
        )
        
        coverage_data = {}
        coverage_file = self.project_root / "coverage.json"
        if coverage_file.exists():
            with open(coverage_file, 'r') as f:
                coverage_data = json.load(f)
        
        return {
            "passed": result.returncode == 0,
            "coverage": coverage_data.get("totals", {}).get("percent_covered", 0),
            "test_count": len(result.stdout.split('\n')),
            "failures": result.stderr.count("FAILED")
        }
    
    def security_scan(self) -> Dict:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³ã‚’å®Ÿè¡Œ"""
        logger.info("ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³ã‚’å®Ÿè¡Œä¸­...")
        
        security_issues = []
        
        # ä¾å­˜é–¢ä¿‚ã®è„†å¼±æ€§ãƒã‚§ãƒƒã‚¯
        result = subprocess.run(
            ["pip", "list", "--format=json"],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            packages = json.loads(result.stdout)
            # å®Ÿéš›ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯APIã‚’å‘¼ã³å‡ºã™ï¼ˆä¾‹ï¼šSnykã€Safetyï¼‰
            # ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼ã®ãƒã‚§ãƒƒã‚¯
            vulnerable_packages = []
            
        # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸèªè¨¼æƒ…å ±ã®ãƒã‚§ãƒƒã‚¯
        secret_patterns = [
            r"password\s*=\s*['\"].*['\"]",
            r"api_key\s*=\s*['\"].*['\"]",
            r"secret\s*=\s*['\"].*['\"]"
        ]
        
        for pattern in secret_patterns:
            result = subprocess.run(
                ["grep", "-r", "-E", pattern, "--include=*.py", "."],
                capture_output=True,
                text=True
            )
            if result.stdout:
                security_issues.append(f"æ½œåœ¨çš„ãªç§˜å¯†æƒ…å ±ã®éœ²å‡º: {pattern}")
        
        security_score = 100 - (len(security_issues) * 5)
        
        return {
            "score": max(0, security_score),
            "issues": security_issues,
            "vulnerable_packages": vulnerable_packages if 'vulnerable_packages' in locals() else []
        }
    
    def performance_test(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        
        # ç°¡å˜ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        import time
        import psutil
        
        start_time = time.time()
        process = psutil.Process()
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼å‡¦ç†
        # å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ†ã‚¹ãƒˆ
        
        end_time = time.time()
        end_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        response_time = (end_time - start_time) * 1000  # ms
        memory_usage = end_memory - start_memory
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«åŸºã¥ãï¼‰
        performance_score = 100
        if response_time > 1000:
            performance_score -= (response_time - 1000) / 100
        if memory_usage > 100:
            performance_score -= (memory_usage - 100) / 10
        
        return {
            "score": max(0, min(100, performance_score)),
            "response_time_ms": response_time,
            "memory_usage_mb": memory_usage,
            "cpu_usage_percent": psutil.cpu_percent(interval=1)
        }
    
    def analyze_code_quality(self) -> Dict:
        """ã‚³ãƒ¼ãƒ‰å“è³ªã‚’åˆ†æ"""
        logger.info("ã‚³ãƒ¼ãƒ‰å“è³ªã‚’åˆ†æä¸­...")
        
        metrics = {
            "complexity": 0,
            "duplications": 0,
            "maintainability": 100,
            "readability": 100
        }
        
        # Flake8ã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯
        result = subprocess.run(
            ["flake8", ".", "--statistics", "--count"],
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            error_count = int(result.stdout.strip()) if result.stdout.strip().isdigit() else 0
            metrics["maintainability"] -= error_count * 2
        
        # å¾ªç’°çš„è¤‡é›‘åº¦ã®è¨ˆç®—ï¼ˆradonã‚’ä½¿ç”¨ï¼‰
        try:
            result = subprocess.run(
                ["radon", "cc", ".", "-a"],
                capture_output=True,
                text=True
            )
            if "Average complexity:" in result.stdout:
                avg_complexity = float(result.stdout.split("Average complexity:")[1].split()[0])
                metrics["complexity"] = avg_complexity
                if avg_complexity > 10:
                    metrics["readability"] -= (avg_complexity - 10) * 5
        except:
            pass
        
        return metrics
    
    def check_documentation(self) -> Dict:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å®Œæˆåº¦ã‚’ãƒã‚§ãƒƒã‚¯"""
        logger.info("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
        
        doc_files = list(self.project_root.glob("**/*.md"))
        py_files = list(self.project_root.glob("**/*.py"))
        
        # Docstringã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’è¨ˆç®—
        total_functions = 0
        documented_functions = 0
        
        for py_file in py_files:
            if "__pycache__" in str(py_file):
                continue
                
            with open(py_file, 'r', encoding='utf-8') as f:
                content = f.read()
                total_functions += content.count("def ")
                documented_functions += content.count('"""')
        
        doc_coverage = (documented_functions / total_functions * 100) if total_functions > 0 else 0
        
        return {
            "markdown_files": len(doc_files),
            "docstring_coverage": doc_coverage,
            "readme_exists": (self.project_root / "README.md").exists(),
            "api_docs_exists": (self.project_root / "docs" / "api.md").exists()
        }
    
    def _calculate_overall_score(self) -> float:
        """å…¨ä½“çš„ãªå“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        scores = []
        
        if 'tests' in self.results:
            scores.append(self.results['tests']['coverage'])
        if 'security' in self.results:
            scores.append(self.results['security']['score'])
        if 'performance' in self.results:
            scores.append(self.results['performance']['score'])
        if 'code_quality' in self.results:
            scores.append(self.results['code_quality']['maintainability'])
        if 'documentation' in self.results:
            scores.append(self.results['documentation']['docstring_coverage'])
        
        return sum(scores) / len(scores) if scores else 0
    
    def _passes_quality_gate(self) -> bool:
        """å“è³ªã‚²ãƒ¼ãƒˆã‚’é€šéã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        if not self.results:
            return False
        
        # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒã‚§ãƒƒã‚¯
        checks = []
        
        if 'tests' in self.results:
            checks.append(self.results['tests']['coverage'] >= self.metrics['code_coverage'])
        if 'security' in self.results:
            checks.append(self.results['security']['score'] >= self.metrics['security_score'])
        if 'performance' in self.results:
            checks.append(self.results['performance']['score'] >= self.metrics['performance_score'])
        
        return all(checks) if checks else False
    
    def deploy_to_production(self):
        """æœ¬ç•ªç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤"""
        logger.info("ğŸš€ æœ¬ç•ªç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã‚’é–‹å§‹ã—ã¾ã™")
        
        # ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
        deploy_script = self.project_root / "scripts" / "deploy.sh"
        if deploy_script.exists():
            subprocess.run(["bash", str(deploy_script)])
        
        # ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†ã®é€šçŸ¥
        self._send_notification("ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†", "å“è³ªã‚²ãƒ¼ãƒˆã‚’é€šéã—ã€æœ¬ç•ªç’°å¢ƒã¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    def create_improvement_tasks(self):
        """æ”¹å–„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ"""
        logger.info("ğŸ“‹ æ”¹å–„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã—ã¾ã™")
        
        tasks = []
        
        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®æ”¹å–„
        if self.results.get('tests', {}).get('coverage', 0) < self.metrics['code_coverage']:
            tasks.append({
                "title": "ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®æ”¹å–„",
                "description": f"ç¾åœ¨ã®ã‚«ãƒãƒ¬ãƒƒã‚¸: {self.results['tests']['coverage']:.1f}%ã€ç›®æ¨™: {self.metrics['code_coverage']}%",
                "priority": "high"
            })
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®æ”¹å–„
        if self.results.get('security', {}).get('issues'):
            tasks.append({
                "title": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œã®ä¿®æ­£",
                "description": f"æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {', '.join(self.results['security']['issues'])}",
                "priority": "critical"
            })
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ”¹å–„
        if self.results.get('performance', {}).get('score', 0) < self.metrics['performance_score']:
            tasks.append({
                "title": "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æœ€é©åŒ–",
                "description": f"ç¾åœ¨ã®ã‚¹ã‚³ã‚¢: {self.results['performance']['score']:.1f}ã€ç›®æ¨™: {self.metrics['performance_score']}",
                "priority": "medium"
            })
        
        # ã‚¿ã‚¹ã‚¯ã®ä¿å­˜
        tasks_file = self.project_root / "improvement_tasks.json"
        with open(tasks_file, 'w') as f:
            json.dump(tasks, f, indent=2)
        
        logger.info(f"{len(tasks)}å€‹ã®æ”¹å–„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸ")
    
    def _send_notification(self, title: str, message: str):
        """é€šçŸ¥ã‚’é€ä¿¡"""
        logger.info(f"é€šçŸ¥: {title} - {message}")
        # Slackã€ãƒ¡ãƒ¼ãƒ«ã€ãã®ä»–ã®é€šçŸ¥ã‚µãƒ¼ãƒ“ã‚¹ã¨ã®çµ±åˆ
    
    def generate_report(self) -> str:
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = f"""
# å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## å…¨ä½“ã‚¹ã‚³ã‚¢: {self.results.get('overall_score', 0):.1f}/100

### ãƒ†ã‚¹ãƒˆçµæœ
- ã‚«ãƒãƒ¬ãƒƒã‚¸: {self.results.get('tests', {}).get('coverage', 0):.1f}%
- ãƒ†ã‚¹ãƒˆæˆåŠŸ: {'âœ…' if self.results.get('tests', {}).get('passed') else 'âŒ'}

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
- ã‚¹ã‚³ã‚¢: {self.results.get('security', {}).get('score', 0)}/100
- æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {len(self.results.get('security', {}).get('issues', []))}

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- ã‚¹ã‚³ã‚¢: {self.results.get('performance', {}).get('score', 0):.1f}/100
- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ : {self.results.get('performance', {}).get('response_time_ms', 0):.1f}ms

### ã‚³ãƒ¼ãƒ‰å“è³ª
- ä¿å®ˆæ€§: {self.results.get('code_quality', {}).get('maintainability', 0)}/100
- å¯èª­æ€§: {self.results.get('code_quality', {}).get('readability', 0)}/100

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- Docstringã‚«ãƒãƒ¬ãƒƒã‚¸: {self.results.get('documentation', {}).get('docstring_coverage', 0):.1f}%
- READMEã®å­˜åœ¨: {'âœ…' if self.results.get('documentation', {}).get('readme_exists') else 'âŒ'}
"""
        
        report_file = self.project_root / "quality_report.md"
        with open(report_file, 'w') as f:
            f.write(report)
        
        return report


if __name__ == "__main__":
    monitor = QualityGate()
    results = monitor.automated_review()
    report = monitor.generate_report()
    print(report)